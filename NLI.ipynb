{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rnanclarez/scripts_mestrado/blob/main/NLI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLwXKUmIptg4"
      },
      "source": [
        "Implementações parcialmente baseadas nas seguintes fontes\n",
        "https://www.analyticsvidhya.com/blog/2021/05/bert-for-natural-language-inference-simplified-in-pytorch/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0X6CuWvj99PV"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xEKu_HLzB1p"
      },
      "outputs": [],
      "source": [
        "### informação do tipo de processador usado(cpu/gpu...)\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "### informação de uso de memória\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsXCqEDYzYfR"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZBarsA33kaa"
      },
      "outputs": [],
      "source": [
        "%%script false\n",
        "!pip install -U torch==1.10.0 torchtext==0.11.0\n",
        "!cp -r /content/drive/MyDrive/mestrado/colab/kfold ./kfold\n",
        "!cp -r /content/drive/MyDrive/mestrado/colab/nlisint ./nlisint\n",
        "!pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nlisint import utils as ut"
      ],
      "metadata": {
        "id": "crGqGcfrkugx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozX9Oqhz4gyg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hc7AAZ0mqybv"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM, get_constant_schedule_with_warmup\n",
        "\n",
        "# OPTIONAL: if you want to have more information on what's happening under the hood, activate the logger as follows\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O88Rtts4uE9_"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained model (weights)\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tcVrr7EE2oG2"
      },
      "outputs": [],
      "source": [
        "cls_token = ut.tokenizer.cls_token\n",
        "sep_token = ut.tokenizer.sep_token\n",
        "pad_token = ut.tokenizer.pad_token\n",
        "unk_token = ut.tokenizer.unk_token\n",
        "#print(cls_token, sep_token, pad_token, unk_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kvuDiwSd2rpg"
      },
      "outputs": [],
      "source": [
        "cls_token_idx = ut.tokenizer.cls_token_id\n",
        "sep_token_idx = ut.tokenizer.sep_token_id\n",
        "pad_token_idx = ut.tokenizer.pad_token_id\n",
        "unk_token_idx = ut.tokenizer.unk_token_id\n",
        "#print(cls_token_idx, sep_token_idx, pad_token_idx, unk_token_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RyJ6yrKM0kCB"
      },
      "outputs": [],
      "source": [
        "#For latest version use torchtext.legacy\n",
        "from torchtext.legacy import data\n",
        "#For sequence\n",
        "TEXT = data.Field(batch_first = True,\n",
        "                  use_vocab = False,\n",
        "                  tokenize = ut.split_and_cut,\n",
        "                  preprocessing = ut.tokenizer.convert_tokens_to_ids,\n",
        "                  pad_token = pad_token_idx,\n",
        "                  unk_token = unk_token_idx)\n",
        "#For label\n",
        "LABEL = data.LabelField()\n",
        "#For Attention mask\n",
        "ATTENTION = data.Field(batch_first = True,\n",
        "                       use_vocab = False,\n",
        "                       tokenize = ut.split_and_cut,\n",
        "                       preprocessing = ut.convert_to_int,\n",
        "                       pad_token = pad_token_idx)\n",
        "#For token type ids\n",
        "TTYPE = data.Field(batch_first = True,\n",
        "                   use_vocab = False,\n",
        "                   tokenize = ut.split_and_cut,\n",
        "                   preprocessing = ut.convert_to_int,\n",
        "                   pad_token = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7xbvPZ1o23UM"
      },
      "outputs": [],
      "source": [
        "fields = [('label', LABEL), ('sequence', TEXT), ('attention_mask', ATTENTION), ('token_type', TTYPE)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "PRjPzuwnOm4X"
      },
      "outputs": [],
      "source": [
        "FOLD = 5\n",
        "#print(f'snli_1.0_train_k{FOLD}.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ve7Oa2Ij28el"
      },
      "outputs": [],
      "source": [
        "#train_data, valid_data, test_data = data.TabularDataset.splits(path = '/kfold/coliee',\n",
        "#train_data, valid_data = data.TabularDataset.splits(path = '/content/kfold/coliee',\n",
        "train_data, valid_data = data.TabularDataset.splits(path = '/content/kfold/snli_binario',\n",
        "\n",
        "                                        train = f'snli_bi_1.0_train_k{FOLD}.csv',\n",
        "                                        #train = f'coliee4_train_k{FOLD}.csv',\n",
        "                                        validation = f'snli_bi_1.0_valid_k{FOLD}.csv',\n",
        "                                        #validation = f'coliee4_valid_k{FOLD}.csv',\n",
        "                                        #test = 'snli_1.0_test_full.csv',\n",
        "                                        format = 'csv',\n",
        "                                        fields = fields,\n",
        "                                        skip_header = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "iF5LOEMA3d0Y"
      },
      "outputs": [],
      "source": [
        "LABEL.build_vocab(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "hAjk1E2-3kR7"
      },
      "outputs": [],
      "source": [
        "#Create iterator\n",
        "BATCH_SIZE = 32\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
        "#   (train_data, valid_data, test_data), \n",
        "    (train_data, valid_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_key = lambda x: len(x.sequence),\n",
        "    sort_within_batch = False, \n",
        "    device = device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "q8UipiEX3ubM"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "class BERTNLIModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert_model,\n",
        "                 hidden_dim,\n",
        "                 output_dim,):\n",
        "        super().__init__()\n",
        "        self.bert = bert_model\n",
        "\n",
        "        embedding_dim = bert_model.config.to_dict()['hidden_size']\n",
        "        self.out = nn.Linear(embedding_dim, output_dim)\n",
        "    def forward(self, sequence, attn_mask, token_type):\n",
        "        embedded = self.bert(input_ids = sequence, attention_mask = attn_mask, token_type_ids= token_type)[1]\n",
        "        output = self.out(embedded)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Bo80Rc_934oL"
      },
      "outputs": [],
      "source": [
        "#defining model\n",
        "HIDDEN_DIM = 512\n",
        "OUTPUT_DIM = len(LABEL.vocab)\n",
        "model = BERTNLIModel(bert_model, HIDDEN_DIM, OUTPUT_DIM, ).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "LuN0b2D_7qhG"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "#optimizer = optim.AdamW(model.parameters(),lr=2e-5,eps=1e-6,correct_bias=False)\n",
        "optimizer = optim.AdamW(model.parameters(),lr=2e-5,eps=1e-6)\n",
        "def get_scheduler(optimizer, warmup_steps):\n",
        "    scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps)\n",
        "    return scheduler\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "#mp = True\n",
        "mp = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "f1X86WgG8fRZ"
      },
      "outputs": [],
      "source": [
        "#!git clone https://github.com/NVIDIA/apex\n",
        "#!!cd apex\n",
        "#!pip install -v --disable-pip-version-check --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./\n",
        "#!cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Iz3NUqdB7m_h"
      },
      "outputs": [],
      "source": [
        "#if mp:\n",
        "#    try:\n",
        "#        from apex import amp\n",
        "#    except ImportError:\n",
        "#        raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
        "#    model, optimizer = amp.initialize(model, optimizer, opt_level='O1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "K7tEBQ3q86G6"
      },
      "outputs": [],
      "source": [
        "def categorical_accuracy(preds, y):\n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True)\n",
        "\n",
        "    correct = (max_preds.squeeze(1)==y).float()\n",
        "\n",
        "    return correct.sum() / len(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "RxyAgjha9EIT"
      },
      "outputs": [],
      "source": [
        "max_grad_norm = 1\n",
        "def train(model, iterator, optimizer, criterion, scheduler):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  model.train()\n",
        "  for batch in iterator:\n",
        "    optimizer.zero_grad() # clear gradients first\n",
        "    torch.cuda.empty_cache() # releases all unoccupied cached memory\n",
        "    sequence = batch.sequence\n",
        "    attn_mask = batch.attention_mask\n",
        "    token_type = batch.token_type\n",
        "    label = batch.label\n",
        "    predictions = model(sequence, attn_mask, token_type)\n",
        "    loss = criterion(predictions, label)\n",
        "    acc = categorical_accuracy(predictions, label)\n",
        "    if mp:\n",
        "      with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "        scaled_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), max_grad_norm)\n",
        "    else:\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "      epoch_loss += loss.item()\n",
        "      epoch_acc += acc.item()\n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "qbLez3ib9t6C"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for batch in iterator:\n",
        "      sequence = batch.sequence\n",
        "      attn_mask = batch.attention_mask\n",
        "      token_type = batch.token_type\n",
        "      labels = batch.label\n",
        "      predictions = model(sequence, attn_mask, token_type)\n",
        "      loss = criterion(predictions, labels)\n",
        "      acc = categorical_accuracy(predictions, labels)\n",
        "      epoch_loss += loss.item()\n",
        "      epoch_acc += acc.item()\n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "kxMoWvGU-YP7"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "def epoch_time(start_time, end_time):\n",
        "  elapsed_time = end_time - start_time\n",
        "  elapsed_mins = int(elapsed_time / 60)\n",
        "  elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "  return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "SUVosjEJ_bLh"
      },
      "outputs": [],
      "source": [
        "train_data_len = 335680"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "mXc10vuUCe0a"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "N_EPOCHS = 5\n",
        "warmup_percent = 0.2\n",
        "total_steps = math.ceil(N_EPOCHS*train_data_len*1./BATCH_SIZE)\n",
        "warmup_steps = int(total_steps*warmup_percent)\n",
        "scheduler = get_scheduler(optimizer, warmup_steps)\n",
        "best_valid_loss = float('inf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ir-wqN4w-fsE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64ad3f15-7155-45bf-85de-f3c224020d04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold 5\n"
          ]
        }
      ],
      "source": [
        "print(f'fold {FOLD}')\n",
        "for epoch in range(N_EPOCHS):\n",
        "  start_time = time.time()\n",
        "  train_loss, train_acc = train(model, train_iterator, optimizer, criterion, scheduler)\n",
        "  valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "  end_time = time.time()\n",
        "  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "  if valid_loss < best_valid_loss:\n",
        "    best_valid_loss = valid_loss\n",
        "    torch.save(model.state_dict(), 'bert-nli.pt')\n",
        "  print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "  print(f'tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "  print(f't Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "VP7SKZKdVQg4"
      },
      "outputs": [],
      "source": [
        "!cp ./bert-nli.pt /content/drive/MyDrive/mestrado/colab/modelosTreinados/bert_snli_BI_k04.pt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/mestrado/colab/modelosTreinados/bert_snli_BI_k4.pt ."
      ],
      "metadata": {
        "id": "dyHSo9RGkCLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxOWRMnXAiUw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54f4666b-6bdf-4aad-b535-5a18e5eec417"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.459 |  Test Acc: 52.82%\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('/content/drive/MyDrive/mestrado/colab/modelosTreinados/bert_snli_BI_k4.pt'))\n",
        "#model.load_state_dict(torch.load('bert-nli.pt',map_location=torch.device('cpu') ))\n",
        "#model.load_state_dict(torch.load('bert_snli_BI_k4.pt'))\n",
        "#test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "test_loss, test_acc = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ro3VnjEPAmCR"
      },
      "outputs": [],
      "source": [
        "def predict_inference(premise, hypothesis, model, device):\n",
        "    model.eval()\n",
        "    premise = '[CLS] ' + premise + ' [SEP]'\n",
        "    hypothesis = hypothesis + ' [SEP]'\n",
        "    prem_t = ut.tokenize_bert(premise)\n",
        "    hypo_t = ut.tokenize_bert(hypothesis)\n",
        "    prem_type = ut.get_sent1_token_type(prem_t)\n",
        "    hypo_type = ut.get_sent2_token_type(hypo_t)\n",
        "    indexes = prem_t + hypo_t\n",
        "    indexes = ut.tokenizer.convert_tokens_to_ids(indexes)\n",
        "    indexes_type = prem_type + hypo_type\n",
        "    attn_mask = ut.get_sent2_token_type(indexes)\n",
        "    indexes = torch.LongTensor(indexes).unsqueeze(0).to(device)\n",
        "    indexes_type = torch.LongTensor(indexes_type).unsqueeze(0).to(device)\n",
        "    attn_mask = torch.LongTensor(attn_mask).unsqueeze(0).to(device)\n",
        "    prediction = model(indexes, attn_mask, indexes_type)\n",
        "    prediction = prediction.argmax(dim=-1).item()\n",
        "    return LABEL.vocab.itos[prediction]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SjGOWK-zApgk"
      },
      "outputs": [],
      "source": [
        "premise = 'john and his brother is wallking in the park'\n",
        "hypothesis = 'john has a sister'\n",
        "predict_inference(premise, hypothesis, model, device)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "NLI.ipynb",
      "provenance": [],
      "mount_file_id": "1peHmlmsVBq5AYMd2b9lnBJscKla0C9Gx",
      "authorship_tag": "ABX9TyMeHOY9Jbr4gI3YZw492HTa",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}