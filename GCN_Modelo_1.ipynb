{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rnanclarez/scripts_mestrado/blob/main/GCN_Modelo_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0X6CuWvj99PV"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#%env CUDA_LAUNCH_BLOCKING=1\n",
        "%env CUBLAS_WORKSPACE_CONFIG=:4096:8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1xEKu_HLzB1p"
      },
      "outputs": [],
      "source": [
        "#@title Teste de memória\n",
        "### informação do tipo de processador usado(cpu/gpu...)\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "### informação de uso de memória\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNnXQUew6fII"
      },
      "outputs": [],
      "source": [
        "#@title setUp do ambiente\n",
        "!python -c \"import torch; print(torch.__version__)\"\n",
        "\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "print(locale.getpreferredencoding())\n",
        "\n",
        "!pip install transformers\n",
        "!pip install torchdata\n",
        "\n",
        "#ajustar a verão do pygeometric com a versão do python e cuda instalada\n",
        "!pip install pyg-lib torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
        "!pip install torch-geometric\n",
        "\n",
        "#!pip install onnx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "ozX9Oqhz4gyg"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset #, DataLoader\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import NodeLoader , DataLoader\n",
        "\n",
        "import pprint\n",
        "\n",
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM, get_constant_schedule_with_warmup, PreTrainedTokenizerFast\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.sparse as sps\n",
        "from sklearn import preprocessing \n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "import os\n",
        "import gc \n",
        "\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "#import torch.onnx\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O88Rtts4uE9_",
        "outputId": "75a73e29-4177-48b4-d734-2a0f9fb3123e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "#@title Carregamento do Bert e do spacy\n",
        "\n",
        "# Load pre-trained model (weights)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "TU5_O9Nl8E-C"
      },
      "outputs": [],
      "source": [
        "#@title Configurações { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "N_FOLDS = 10 #@param {type:\"number\"}\n",
        "\n",
        "BATCH_SIZE = 8 #@param {type:\"number\"}\n",
        "N_EPOCHS = 5 #@param {type:\"number\"}\n",
        "warmup_percent = 0.2 #@param {type:\"number\"}\n",
        "FOLDER = 'folds' #@param {type:\"string\"}\n",
        "PREFIXO = 'coliee_data' #@param {type:\"string\"}\n",
        "OUTPUT_DIM = 2 #@param {type:\"number\"}\n",
        "device_form = \"CUDA\" #@param [\"CPU\", \"CUDA\"]\n",
        "\n",
        "torch.use_deterministic_algorithms(True)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() and device_form == 'CUDA' else 'cpu')\n",
        "#device = torch.device('cpu')\n",
        "criterion = nn.CrossEntropyLoss().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "z84SjLyOzqQo"
      },
      "outputs": [],
      "source": [
        "#@title Funções\n",
        "\n",
        "def coo_matrix(texto):\n",
        "  doc = nlp(texto)\n",
        "  return zip(*[(child.head.i, child.i) for token in doc for child in token.children])\n",
        "\n",
        "def join_sentences(sent1, sent2):\n",
        "  doc = [tokenizer.cls_token.strip('[').strip(']')]\n",
        "  doc.extend(tokenizer.tokenize(sent1))\n",
        "  doc.append(tokenizer.sep_token.strip('[').strip(']'))\n",
        "  doc.extend(tokenizer.tokenize(sent2))\n",
        "  doc.append(tokenizer.sep_token.strip('[').strip(']'))\n",
        "  return ' '.join(doc)\n",
        "\n",
        "def calcularMetricas(preds, y, batch, batchSize):\n",
        "  arg_max = preds.argmax(dim = 1).tolist()\n",
        "  corretas = [ int(i[0] == i[1]) for i in zip(arg_max ,y.tolist())]\n",
        "  erradas = [ int(i[0] != i[1]) for i in zip(arg_max ,y.tolist())]\n",
        "  \n",
        "  erros = [(i + batch*batchSize, arg_max[i], y.tolist()[i]) for i, j in enumerate(erradas)  if j == 1]\n",
        "\n",
        "  #return correct.sum() / len(y)\n",
        "  acc = accuracy_score(y.tolist(), arg_max)\n",
        "  prf = precision_recall_fscore_support(y.tolist(), arg_max, average=None, labels=[0, 1])\n",
        "  return acc, *prf, erros\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "  elapsed_time = end_time - start_time\n",
        "  elapsed_mins = int(elapsed_time / 60)\n",
        "  elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "  return elapsed_mins, elapsed_secs\n",
        "\n",
        "def getModelInputs(sent1, sent2):\n",
        "  sent1 = sent1[:255] if len(sent1) >= 256 else sent1\n",
        "  rows, cols = coo_matrix(join_sentences(sent1, sent2))\n",
        "  edge_index = [rows, cols]\n",
        "  \n",
        "  pad_sequences = tokenizer(sent1, sent2, max_length=512, padding='max_length', truncation=True)\n",
        "  \n",
        "  sentences_join = pad_sequences['input_ids']\n",
        "  masks_join =  pad_sequences['attention_mask']\n",
        "  type_join = pad_sequences['token_type_ids']\n",
        "\n",
        "  return  sentences_join, masks_join, type_join, edge_index\n",
        "\n",
        "def getDataLoader(fold, batch, folder, prefixo, tipoDataSet):\n",
        "  dataset = PygDataSet(f'{folder}/{prefixo}_{tipoDataSet}_{fold}.csv', 0)\n",
        "  data_len = len(dataset)\n",
        "  dataloader = DataLoader(dataset=dataset, batch_size=batch)\n",
        "\n",
        "  return data_len, dataloader\n",
        "\n",
        "\n",
        "def get_scheduler(optimizer, warmup_steps):\n",
        "  return get_constant_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps)\n",
        "\n",
        "def unzipBatch(batch, device):\n",
        "  batch_size = len(batch)\n",
        "  sequence = torch.stack(batch.x.transpose(0,1)[0].chunk(batch_size)).to(device) \n",
        "  attn_mask = torch.stack(batch.x.transpose(0,1)[1].chunk(batch_size)).to(device) \n",
        "  token_type = torch.stack(batch.x.transpose(0,1)[2].chunk(batch_size)).to(device) \n",
        "  edge_index = batch.edge_index.to(device)\n",
        "\n",
        "  return sequence, attn_mask, token_type, edge_index\n",
        "\n",
        "def treinarEpoca(model, iterator, optimizer, criterion, scheduler):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  model.train()\n",
        "  for i, batch in enumerate(iterator):\n",
        "    optimizer.zero_grad() # clear gradients first\n",
        "    label = batch.y.type(torch.LongTensor).to(device)\n",
        "    predictions = model(*unzipBatch(batch, device))\n",
        "\n",
        "    loss = criterion(predictions, label)\n",
        "    metricas = calcularMetricas(predictions, label, i, BATCH_SIZE)\n",
        "    print(metricas[5])\n",
        "    print(\"-----\")\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    epoch_loss += loss.detach().item()\n",
        "    epoch_acc += metricas[0]\n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def validarEpoca(model, iterator, criterion):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for i, batch in enumerate(iterator):\n",
        "      label = batch.y.type(torch.LongTensor).to(device)\n",
        "      predictions = model(*unzipBatch(batch, device))\n",
        "\n",
        "      loss = criterion(predictions, label)\n",
        "      metricas = calcularMetricas(predictions, label, i, BATCH_SIZE)\n",
        "      epoch_loss += loss.item()\n",
        "      epoch_acc += metricas[0]\n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def inferir(premissa, hipotese, model, classes):\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() and device_form == 'CUDA' else 'cpu')\n",
        "  model.eval()\n",
        "  \n",
        "  le = preprocessing.LabelEncoder().fit(classes)\n",
        "\n",
        "  inputs = getModelInputs(premissa, hipotese)\n",
        "\n",
        "  prediction = model(torch.tensor(inputs[0])[None, :].to(device),\n",
        "                      torch.tensor(inputs[1])[None, :].to(device),\n",
        "                      torch.tensor(inputs[2])[None, :].to(device),\n",
        "                      torch.tensor(inputs[3]).to(device))\n",
        "  print(prediction)\n",
        "  prediction = prediction.argmax(dim=-1).item()\n",
        "  print(prediction)\n",
        "  return le.inverse_transform([prediction])[0]\n",
        "\n",
        "\n",
        "def salvarOnnx():\n",
        "\n",
        "  valid_data_len,  valid_dataloader = getDataLoader(0, BATCH_SIZE, FOLDER, PREFIXO, 'valid')\n",
        "\n",
        "  batch = next(iter(valid_dataloader))\n",
        "  label = batch.y.type(torch.LongTensor).to(device)\n",
        "  bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "  model = GCNBERTNLIModel(bert_model, OUTPUT_DIM).to(device)\n",
        "  model.eval()\n",
        "  inputs = unzipBatch(batch, device)\n",
        "  predictions = model(inputs)\n",
        "  torch.onnx.export(model, inputs, \"model.onnx\" , export_params=True, opset_version=10, do_constant_folding=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "KH0VRvx1Eebh"
      },
      "outputs": [],
      "source": [
        "#@title Class PygDataSet\n",
        "class PygDataSet(Dataset):\n",
        "    def __init__(self, arquivo, skiprows=1):\n",
        "        self.df = pd.read_csv(arquivo, skiprows = skiprows )\n",
        "        self.df.dropna(axis=0, how='any', inplace=True) #remove linhas com colunas vazias\n",
        "        self.le = preprocessing.LabelEncoder()\n",
        "        self.le.fit(self.df.iloc[:,1])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sent1 = self.df.iloc[idx,2]\n",
        "        sent2 = self.df.iloc[idx,3]\n",
        "\n",
        "        sentences_join, masks_join, type_join, edge_index = getModelInputs(sent1, sent2)\n",
        "        x = torch.tensor([sentences_join,masks_join,type_join]).transpose(0,1)\n",
        "        indices = torch.tensor(edge_index)\n",
        "        labels = self.le.transform([self.df.iloc[idx,1]])\n",
        "        return  Data(edge_index=indices, x=x, y=torch.tensor(labels))\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ImjRbZqu3I4Q",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#%script false --no-raise-error\n",
        "#@title carrega dados coliee\n",
        "\n",
        "!cp -r /content/drive/MyDrive/mestrado/coliee_data/. ./coliee_data\n",
        "\n",
        "task4df = pd.DataFrame( columns = ['label', 't1', 't2'])\n",
        "\n",
        "\n",
        "# import required module\n",
        "import os\n",
        "# assign directory\n",
        "directory = '/content/coliee_data/task3_4/COLIEE2021statute_data-English/train'\n",
        " \n",
        "# iterate over files in\n",
        "# that directory\n",
        "for filename in os.listdir(directory):\n",
        "    f = os.path.join(directory, filename)\n",
        "    # checking if it is a file\n",
        "    if os.path.isfile(f):\n",
        "        task4df = pd.concat([task4df,pd.read_xml(f)])\n",
        "#task4df.to_csv(\"coliee_data.csv\")\n",
        "kf = KFold(n_splits=10, shuffle=False)\n",
        "!mkdir folds\n",
        "for i, (t_ind, v_ind) in enumerate(kf.split(task4df)):\n",
        "    \n",
        "    #print(t_ind, v_ind)\n",
        "    task4df.iloc[t_ind].to_csv(f'folds/coliee_data_train_{i}.csv')     # train set\n",
        "    task4df.iloc[v_ind].to_csv(f'folds/coliee_data_valid_{i}.csv')    # validation set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWwj6f_N1n1h"
      },
      "outputs": [],
      "source": [
        "#@title carrega dados SNLI\n",
        "#%%script false\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/mestrado/colab/snli_1.0/snli_1.0_train.txt', sep='\\t')\n",
        "\n",
        "df = pd.concat([df,pd.read_csv('/content/drive/MyDrive/mestrado/colab/snli_1.0/snli_1.0_dev.txt', sep='\\t')])\n",
        "\n",
        "df = pd.concat([df,pd.read_csv('/content/drive/MyDrive/mestrado/colab/snli_1.0/snli_1.0_test.txt', sep='\\t')])\n",
        "\n",
        "#df[\"id\"] = df.index + 1\n",
        "\n",
        "df = df[['gold_label','sentence1','sentence2']]\n",
        "\n",
        "df = df.loc[df[\"gold_label\"] != '-' ]\n",
        "\n",
        "kf = KFold(n_splits=10, shuffle=False)\n",
        "\n",
        "!mkdir folds\n",
        "\n",
        "for i, (t_ind, v_ind) in enumerate(kf.split(df)):\n",
        "    \n",
        "    df.iloc[t_ind].to_csv(f'folds/snli_data_train_{i}.csv')   # train set\n",
        "    df.iloc[v_ind].to_csv(f'folds/snli_data_valid_{i}.csv')   # validation set\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "iVOlQL34GVOc",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Modelo_1\n",
        "class GCNBERTNLIModel(nn.Module):\n",
        "    def __init__(self, bert_model, output_dim):\n",
        "        super().__init__()\n",
        "        self.bert = bert_model\n",
        "        self.conv1 = GCNConv(768, 768)\n",
        "        self.conv2 = GCNConv(768, 768)\n",
        "\n",
        "        embedding_dim = bert_model.config.to_dict()['hidden_size']\n",
        "\n",
        "        self.out = nn.Linear(768 * 512, 256)\n",
        "        self.out2 = nn.Linear(256, output_dim)\n",
        "\n",
        "    def forward(self, sequence, attn_mask, token_type, edge_index):\n",
        "        embedded = self.bert(input_ids = sequence, attention_mask = attn_mask, token_type_ids= token_type)\n",
        "        shape = embedded[0].shape\n",
        "        x = self.conv1(torch.reshape(embedded[0],(shape[0] * shape[1],shape[2])), edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "\n",
        "        #x = self.conv2(x, edge_index)\n",
        "        x = torch.reshape(x, shape)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        output = self.out(x)\n",
        "\n",
        "        output = self.out2(output)\n",
        "\n",
        "        return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ir-wqN4w-fsE"
      },
      "outputs": [],
      "source": [
        "#@title Loop de treinamento\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "for f in range(N_FOLDS):\n",
        "  train_data_len,  train_dataloader = getDataLoader(f, BATCH_SIZE, FOLDER, PREFIXO, 'train')\n",
        "  valid_data_len,  valid_dataloader = getDataLoader(f, BATCH_SIZE, FOLDER, PREFIXO, 'valid')\n",
        "\n",
        "  torch.manual_seed(42)\n",
        "  bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "  model = GCNBERTNLIModel(bert_model, OUTPUT_DIM).to(device)\n",
        "  torch.cuda.empty_cache() # releases all unoccupied cached memory\n",
        "\n",
        "  optimizer = optim.AdamW(model.parameters(),lr=2e-5,eps=1e-6)\n",
        "  total_steps = math.ceil(N_EPOCHS*train_data_len*1./BATCH_SIZE)\n",
        "  warmup_steps = int(total_steps*warmup_percent)\n",
        "  scheduler = get_scheduler(optimizer, warmup_steps)\n",
        "  best_valid_loss = float('inf')\n",
        "  print(f'Fold - {f}')\n",
        "\n",
        "  for epoch in range(N_EPOCHS):\n",
        "    gc.collect()\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = treinarEpoca(model, train_dataloader, optimizer, criterion, scheduler)\n",
        "    valid_loss, valid_acc = validarEpoca(model, valid_dataloader, criterion)\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    if valid_loss < best_valid_loss:\n",
        "      best_valid_loss = valid_loss\n",
        "      torch.save(model.state_dict(), 'bert-nli.pt')\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f't Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VP7SKZKdVQg4"
      },
      "outputs": [],
      "source": [
        "#@title Salvar/Carregar modelos\n",
        "\n",
        "#from google.colab import files\n",
        "#files.download('model.onnx') \n",
        "!cp model.onnx /content/drive/MyDrive/mestrado/model.onnx\n",
        "\n",
        "#!cp ./bert-nli.pt /content/drive/MyDrive/mestrado/colab/modelosTreinados/bert_snli_BI_k04.pt\n",
        "!cp /content/drive/MyDrive/mestrado/colab/modelosTreinados/bert_snli_BI_k02.pt ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjGOWK-zApgk"
      },
      "outputs": [],
      "source": [
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "model = GCNBERTNLIModel(bert_model, OUTPUT_DIM).to(device)\n",
        "premise = 'john and his brother is wallking in the park'\n",
        "hypothesis = 'john is has a sister'\n",
        "inferir(premise, hypothesis, model, ['Y','N'])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}